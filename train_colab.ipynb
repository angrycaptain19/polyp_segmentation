{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!sudo apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install thop\n",
    "!unzip -qq '/content/gdrive/My Drive/pranet/data/Kvasir_fold_new.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "%cd /content/gdrive/MyDrive/pranet\n",
    "from utils.dataloader import get_loader\n",
    "from utils.utils import clip_gradient, adjust_lr, AvgMeter\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.logger import Logger as Log\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, img_paths, mask_paths, aug=True, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.aug = aug\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        image = imread(img_path)\n",
    "        mask = imread(mask_path)\n",
    "        image = cv2.resize(image, (352, 352))\n",
    "\n",
    "        image = image.astype('float32') / 255\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        mask = mask[:,:,np.newaxis]\n",
    "        \n",
    "        mask = mask.astype('float32')\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "\n",
    "        return np.asarray(image), np.asarray(mask)\n",
    "  \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, img_paths, mask_paths, aug=True, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.aug = aug\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        image = imread(img_path)\n",
    "        mask = imread(mask_path)\n",
    "\n",
    "        if self.aug:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = image.astype('float32') / 255\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        mask = mask[:,:,np.newaxis]\n",
    "        \n",
    "        mask = mask.astype('float32')\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "        return np.asarray(image), np.asarray(mask)\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1)/(union - inter+1)\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "\n",
    "# criterion1 = OhemCrossEntropy2dTensor(thresh=0.7, min_kept=100000)\n",
    "# criterion1 = torch.nn.CrossEntropyLoss(reduction='elementwise_mean')\n",
    "\n",
    "def train(train_loader,test_loader, model, optimizer, epoch, test_fold,writer):\n",
    "    test_size = len(test_loader)\n",
    "\n",
    "    model.train()\n",
    "    # ---- multi-scale training ----6\n",
    "    size_rates = [0.75, 1, 1.25]\n",
    "\n",
    "    loss_recordx4,\\\n",
    "    loss_recordx3,\\\n",
    "    loss_recordx2,\\\n",
    "    loss_record2, loss_record3, loss_record4, loss_record5 = AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter()\n",
    "    # loss_record2_new, loss_record3_new, loss_record4_new, loss_record5_new, loss_record1_new, loss_record0_new  = AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter(),AvgMeter(),AvgMeter()\t\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        for rate in size_rates:\n",
    "            optimizer.zero_grad()\n",
    "            # ---- data prepare ----\n",
    "            images, gts = pack\n",
    "            images = Variable(images).cuda()\n",
    "            gts = Variable(gts).cuda()\n",
    "            # ---- rescale ----\n",
    "            trainsize = int(round(trainsize_init*rate/32)*32)\n",
    "            if rate != 1:\n",
    "                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            # ---- forward ----\n",
    "            # (x4_head_out,\\\n",
    "            x3_head_out = model(images)\n",
    "            # x2_head_out,\\\n",
    "            \n",
    "\n",
    "\n",
    "#             lateral_map_5, lateral_map_4, lateral_map_3, lateral_map_2, new_lateral_map_5, new_lateral_map_4, new_lateral_map_3, new_lateral_map_2, new_lateral_map_1, new_lateral_map_0 = model(images)\n",
    "\n",
    "    \n",
    "            # ---- loss function ----\n",
    "            # lossx4 = structure_loss(x4_head_out, gts)\n",
    "            lossx3 = structure_loss(x3_head_out, gts)\n",
    "            # lossx2 = structure_loss(x2_head_out, gts)\n",
    "            # loss5 = structure_loss(lateral_map_5, gts)\n",
    "            # loss4 = structure_loss(lateral_map_4, gts)\n",
    "            # loss3 = structure_loss(lateral_map_3, gts)\n",
    "            # loss2 = structure_loss(lateral_map_2, gts)\n",
    "            # loss = loss2 + loss3 + loss4 + loss5\n",
    "            # loss += lossx4*0.5\n",
    "            loss = lossx3\n",
    "            # loss += lossx2*0.5   # TODO: try different weights for loss\n",
    "\n",
    "\n",
    "#             loss = loss2 + loss3 + loss4 + loss5    # TODO: try different weights for loss\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "#             loss5_new = structure_loss(new_lateral_map_5, gts)\t\t           \n",
    "#             loss4_new = structure_loss(new_lateral_map_4, gts)\t\n",
    "#             loss3_new = structure_loss(new_lateral_map_3, gts)\t\t\n",
    "#             loss2_new = structure_loss(new_lateral_map_2, gts)\t\t\n",
    "#             loss1_new = structure_loss(new_lateral_map_1, gts)\t\t\n",
    "#             loss0_new = structure_loss(new_lateral_map_0, gts)\t\t\n",
    "#             loss = loss2 + loss3 + loss4 + loss5 + loss5_new + loss4_new + loss3_new + loss2_new + loss1_new + loss0_new   # TODO: try different weights for loss\t\n",
    "            \n",
    "    \n",
    "            # ---- backward ----\n",
    "            loss.backward()\n",
    "            clip_gradient(optimizer, clip)\n",
    "            optimizer.step()\n",
    "            # ---- recording loss ----\n",
    "            if rate == 1:\n",
    "                # loss_recordx4.update(lossx4.data, batchsize)\n",
    "                loss_recordx3.update(lossx3.data, batchsize)\n",
    "                # loss_recordx2.update(lossx2.data, batchsize)\n",
    "                # loss_record2.update(loss2.data, batchsize)\n",
    "                # loss_record3.update(loss3.data, batchsize)\n",
    "                # loss_record4.update(loss4.data, batchsize)\n",
    "                # loss_record5.update(loss5.data, batchsize)\n",
    "\n",
    "                \n",
    "#                 loss_record5_new.update(loss5_new.data, batchsize)\t\t\n",
    "#                 loss_record4_new.update(loss4_new.data, batchsize)\t\t\n",
    "#                 loss_record3_new.update(loss3_new.data, batchsize)\t\t\n",
    "#                 loss_record2_new.update(loss2_new.data, batchsize)\t\t\n",
    "#                 loss_record1_new.update(loss1_new.data, batchsize)\t\t\n",
    "#                 loss_record0_new.update(loss0_new.data, batchsize)\t\n",
    "                \n",
    "                \n",
    "                # writer.add_scalar(\"Loss0\", loss_record1.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"loss_recordx4\", loss_recordx4.show(), (epoch-1)*len(train_loader) + i)\n",
    "                writer.add_scalar(\"loss_recordx3\", loss_recordx3.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"loss_recordx2\", loss_recordx2.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"Loss1\", loss_record2.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"Loss2\", loss_record3.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"Loss3\", loss_record4.show(), (epoch-1)*len(train_loader) + i)\n",
    "                # writer.add_scalar(\"Loss4\", loss_record5.show(), (epoch-1)*len(train_loader) + i)\n",
    "                \n",
    "\n",
    "#                 writer.add_scalar(\"loss_record5_new\", loss_record5_new.show(), (epoch-1)*len(train_loader) + i)\t\t\n",
    "#                 writer.add_scalar(\"loss_record4_new\", loss_record4_new.show(), (epoch-1)*len(train_loader) + i)\t\t\n",
    "#                 writer.add_scalar(\"loss_record3_new\", loss_record3_new.show(), (epoch-1)*len(train_loader) + i)\t\t\n",
    "#                 writer.add_scalar(\"loss_record2_new\", loss_record2_new.show(), (epoch-1)*len(train_loader) + i)\t\t\n",
    "#                 writer.add_scalar(\"loss_record1_new\", loss_record1_new.show(), (epoch-1)*len(train_loader) + i)\t\t\n",
    "#                 writer.add_scalar(\"loss_record0_new\", loss_record0_new.show(), (epoch-1)*len(train_loader) + i)\t\n",
    "\n",
    "\n",
    "        # ---- train visualization ----\n",
    "#         if i == total_step:\n",
    "#             loss_all = 0\n",
    "\n",
    "#             for i, pack_test in enumerate(test_loader, start=1):\n",
    "\n",
    "#                 images, gts = pack_test\n",
    "#                 images = images.cuda()\n",
    "#                 gts = gts.cuda()\n",
    "#                 testsize = 352\n",
    "#                 images = F.upsample(images, size=(testsize, testsize), mode='bilinear', align_corners=True)\n",
    "#                 gts = F.upsample(gts, size=(testsize, testsize), mode='bilinear', align_corners=True)\n",
    "\n",
    "#                 x4_head_out, x3_head_out, x2_head_out, lateral_map_5, lateral_map_4, lateral_map_3, lateral_map_2 = model(images)\n",
    "#                 # lossx4 = structure_loss(x4_head_out, gts)\n",
    "#                 lossx3 = structure_loss(x3_head_out, gts)\n",
    "#                 # lossx2 = structure_loss(x2_head_out, gts)\n",
    "#                 loss5 = structure_loss(lateral_map_5, gts)\n",
    "#                 loss4 = structure_loss(lateral_map_4, gts)\n",
    "#                 loss3 = structure_loss(lateral_map_3, gts)\n",
    "#                 loss2 = structure_loss(lateral_map_2, gts)\n",
    "#                 loss = lossx4*0.5 + lossx3*0.5 + lossx2*0.5 + loss2 + loss3 + loss4 + loss5    # TODO: try different weights for loss\n",
    "#                 loss_all += loss\n",
    "\n",
    "        if i % 25 == 0 or i == total_step:\n",
    "          Log.info('{} Epoch [{:03d}/{:03d}], with lr = {}, Step [{:04d}/{:04d}],\\\n",
    "                  [loss_recordx3: {:.4f}]'.\n",
    "                  format(datetime.now(), epoch, epoch, optimizer.param_groups[0][\"lr\"],i, total_step,\\\n",
    "                        #  loss_recordx4.show(),\\\n",
    "                          loss_recordx3.show(),\\\n",
    "                        #  loss_recordx2.show(),\\\n",
    "                          # loss_record2.show(), loss_record3.show(), loss_record4.show(), loss_record5.show()\n",
    "                          ))\n",
    "          \n",
    "\n",
    "#             print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], '\t\t\n",
    "#                   '[lateral-2: {:.4f}, lateral-3: {:0.4f}, lateral-4: {:0.4f}, lateral-5: {:0.4f}, lateral-5_new: {:.4f}, , lateral-4_new: {:.4f}, lateral-3_new: {:.4f}, , lateral-2_new: {:.4f}, , lateral-1_new: {:.4f}, , lateral-0_new: {:.4f}]'.\t\t#                   '[lateral-2: {:.4f}, lateral-3: {:0.4f}, lateral-4: {:0.4f}, lateral-5: {:0.4f}]'.\n",
    "#                   format(datetime.now(), epoch, epoch, i, total_step,\t\t\n",
    "#                          loss_record2.show(), loss_record3.show(), loss_record4.show(), loss_record5.show(), loss_record5_new.show(), loss_record4_new.show(), loss_record3_new.show(), loss_record2_new.show(), loss_record1_new.show(), loss_record0_new.show()))\n",
    "        \n",
    "    \n",
    "    save_path = 'snapshots/{}/'.format(train_save)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    if (epoch+1) % 3 == 0 and epoch > 60:\n",
    "      torch.save({\"model_state_dict\":model.state_dict(), \"lr\":optimizer.param_groups[0][\"lr\"]}, save_path + 'PraNetDG-' + test_fold +'-%d.pth' % epoch)\n",
    "      Log.info('[Saving Snapshot:]'+  save_path + 'PraNetDG-' + test_fold +'-%d.pth' % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.PraNet_Res2Net import PraNet, PraNetAG, PraNetDGv2,PraNetDGv3, PraNetGALD, PraNetv12\n",
    "import os\n",
    "\n",
    "lr = 1e-4\n",
    "batchsize = 16\n",
    "trainsize_init = 352\n",
    "clip = 0.5\n",
    "decay_rate = 0.1\n",
    "decay_epoch = 50\n",
    "import timeit\n",
    "start_from = 0\n",
    "name = [[1,2,3,4], [0,2,3,4], [0,1,3,4], [0,1,2,4], [0,1,2,3]]\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# for i in range(4,5):\n",
    "v = 12\n",
    "i = 2\n",
    "train_save = 'PraNetv{}_Res2Net_kfold'.format(v)\n",
    "save_path = 'snapshots/{}/'.format(train_save)\n",
    "log_file = 'PraNetv{}_Res2Net_fold{}.log'.format(v,i)\n",
    "Log.init(\n",
    "    log_level=\"info\",\n",
    "    log_file=os.path.join(save_path, log_file),\n",
    "    log_format=\"%(asctime)s %(levelname)-7s %(message)s\",\n",
    "    rewrite=False,\n",
    "    stdout_level=\"info\"\n",
    ")\n",
    "# ---- build models ----\n",
    "# torch.cuda.set_device(0)  # set your gpu device\n",
    "model = PraNetv12().cuda()\n",
    "if start_from != 0: \n",
    "  restore_from = \"./snapshots/PraNetv{}_Res2Net_kfold/PraNetGALD-fold{}-{}.pth\".format(v,i,start_from)\n",
    "  saved_state_dict = torch.load(restore_from)[\"model_state_dict\"]\n",
    "  lr = torch.load(restore_from)[\"lr\"]\n",
    "\n",
    "  # new_params = model.state_dict().copy()\n",
    "  # for i in saved_state_dict:\n",
    "  #     i_parts = i.split('.')\n",
    "  #     if not i_parts[0] == 'fc':\n",
    "  #         new_params['.'.join(i_parts[0:])] = saved_state_dict[i]\n",
    "  model.load_state_dict(saved_state_dict, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train1 = 'fold_' + str(name[i][0])\n",
    "train2 = 'fold_' + str(name[i][1])\n",
    "train3 = 'fold_' + str(name[i][2])\n",
    "train4 = 'fold_' + str(name[i][3])\n",
    "test_fold = 'fold' + str(i)\n",
    "train_img_paths =[]\n",
    "train_mask_paths = []\n",
    "train_img_path_1 = glob('/content/Kvasir_fold_new/' + train1 + \"/images/*\")\n",
    "train_img_paths.extend(train_img_path_1)\n",
    "\n",
    "train_img_path_2 = glob('/content/Kvasir_fold_new/' + train2 + \"/images/*\")\n",
    "train_img_paths.extend(train_img_path_2)\n",
    "\n",
    "train_img_path_3 = glob('/content/Kvasir_fold_new/' + train3 + \"/images/*\")\n",
    "train_img_paths.extend(train_img_path_3)\n",
    "\n",
    "train_img_path_4 = glob('/content/Kvasir_fold_new/' + train4 + \"/images/*\")\n",
    "train_img_paths.extend(train_img_path_4)\n",
    "\n",
    "\n",
    "train_mask_path_1 = glob('/content/Kvasir_fold_new/' + train1 + \"/masks/*\")\n",
    "train_mask_paths.extend(train_mask_path_1)\n",
    "\n",
    "train_mask_path_2 = glob('/content/Kvasir_fold_new/' + train2 + \"/masks/*\")\n",
    "train_mask_paths.extend(train_mask_path_2)\n",
    "\n",
    "train_mask_path_3 = glob('/content/Kvasir_fold_new/' + train3 + \"/masks/*\")\n",
    "train_mask_paths.extend(train_mask_path_3)\n",
    "\n",
    "train_mask_path_4 = glob('/content/Kvasir_fold_new/' + train4 + \"/masks/*\")\n",
    "train_mask_paths.extend(train_mask_path_4)\n",
    "\n",
    "train_img_paths.sort()\n",
    "train_mask_paths.sort()\n",
    "\n",
    "train_transform = Compose([\n",
    "        transforms.RandomRotate90(),\n",
    "        transforms.Flip(),\n",
    "        transforms.HueSaturationValue(),\n",
    "        transforms.RandomBrightnessContrast(),\n",
    "        transforms.Transpose(),\n",
    "        OneOf([\n",
    "          transforms.RandomCrop(220,220, p=0.5),\n",
    "          transforms.CenterCrop(220,220, p=0.5)\n",
    "        ], p=0.5),\n",
    "        transforms.Resize(352,352)\n",
    "    ])\n",
    "\n",
    "train_dataset = Dataset(train_img_paths, train_mask_paths, transform=train_transform)\n",
    "\n",
    "# for id in range(0, 2):\n",
    "#   image, mask = train_dataset[id] # get some sample\n",
    "#   image = np.transpose(image, (1,2,0))\n",
    "#   mask = np.transpose(mask, (1,2,0))\n",
    "#   print(np.max(image), np.max(mask))\n",
    "#   visualize(\n",
    "#       image=image.squeeze(),\n",
    "#       # image=image, \n",
    "#       mask=mask[..., 0].squeeze(),\n",
    "#   )\n",
    "# sys.exit(1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=16,\n",
    "      shuffle=True,\n",
    "      pin_memory=True,\n",
    "      drop_last=True)\n",
    "total_step = len(train_loader)\n",
    "\n",
    "\n",
    "data_path = '/content/Kvasir_fold_new/' + 'fold_' + str(i)\n",
    "X_test = glob('{}/images/*'.format(data_path))\n",
    "X_test.sort()\n",
    "y_test = glob('{}/masks/*'.format(data_path))\n",
    "y_test.sort()\n",
    "test_dataset = Dataset_test(X_test, y_test,aug=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=True)\n",
    "\n",
    "# ---- flops and params ----\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr)\n",
    "\n",
    "Log.info(\"#\"*20 + f\"Start Training Fold{i}\" + \"#\"*20)\n",
    "print(\"#\"*20, f\"Start Training Fold{i}\", \"#\"*20)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(start_from, 99):\n",
    "    adjust_lr(optimizer, lr, epoch, decay_rate, decay_epoch)\n",
    "    train(train_loader, test_loader , model, optimizer, epoch, test_fold,writer)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "end = timeit.default_timer()\n",
    "\n",
    "Log.info(\"Training cost: \"+ str(end - start) + 'seconds')\n",
    "\n"
   ]
  }
 ]
}